# Wikipedia 2 Corpus
Tools to extract, transform and clean the Wikipedia texts to a text corpus for self-supervised NLP model training.

## Download Data
Download the raw Wikipedia dump and store it in the `data` directory:
- German language: select the youngest directory from https://dumps.wikimedia.org/dewiki/ and download a file called `dewiki-<yyyymmdd>-pages-articles.xml.bz2` its is about 5.8 GB - we use `dewiki-20220201-pages-articles.xml.bz2`
